---
title: "Predicting MSRP based on Car Features"
author: "Leslie Cervantes Rivera"
date: "2024-10-26"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo=TRUE, 
                      cache=FALSE, 
                      fig.width=5, 
                      fig.height=3,
                      fig.align='center', 
                      eval=eval)
library(dplyr)
library(naniar)
library(tidyr)
library(ggplot2)
library(forcats)
library(corrplot)
library(tidymodels)
library(vip)
```

![image](~/Desktop/varietycars.jpg)
(https://www.creelighting.com/insights/article/7-reasons-led-lighting-is-the-right-choice-for-automotive-dealerships/)
![image](~/Desktop/price_car.jpg)


(https://stock.adobe.com/search?k=car+price+tag)

# Introduction

The purpose of this project is to build a machine learning model to predict the Manufacturer’s Suggested Retail Price (MSRP) of cars based on various features, such as make, year, and engine fuel type. Beyond prediction, this project aims to address two questions:

- What are the most important car features that influence the price of a car?
- Is there a significant price difference between different types of transmissions?

By examining the relationships between the predictors and MSRP, I aim to identify the factors that contribute to the price variations across models and quantify the impact of transmission types on MSRP. 

![image](~/Desktop/MSRP1.pdf){width=30%}

(https://www.cars.com/articles/what-does-msrp-mean-1420690419467/)

## What is MSRP? 

As an adult, I believe it is important to understand how a car’s features affect its MSRP. MSRP serves as the price the original producer of the car suggests the cost to be (https://www.indeed.com/career-advice/career-development/msrp).The main purpose of MSRP is to give distributors and retailers guidelines to maintain prices consistent and affordable for the majority of customers across locations. This also serves as the starting point for negotiations at the dealership.

MSRP not only sets expectations around affordability, financing, and loan terms, but it also affects recurring expenses like insurance. In today’s economy, MSRP has been affected by inflation, supply chain disruptions, and technological advancements. Understanding these factors is crucial for making wise financial decisions, especially for new buyers who want to get the best value for their investment. This project will shed light on the factors that influence MSRP in the automotive market. 

![image](~/Desktop/IMG_2098.JPEG)

## Car Features and Pricing

What is something you look for when buying a car?

The car's features! Car features are the most critical factors influencing the price of the car, as they offer functionality, performance, and appeal. Features like Engine HP and Transmission Type affect how the car performs and driving experience, while other features such as Vehicle Style and Market Category offer appealance. Luxury cars tend to have advanced features like engine horsepower, transmission type, and more, which significantly drive up the MSRP. Conversely, non-luxury cars focus more on affordability. This project will explore the relationship between the features and the MSRP, seeking to determine which features are most significant. 

## Transmission Types and Pricing 



## Data

The data that will be used for this project contains detailed information about cars, including features, such as make, mode, year, engine specifications, MSRP, and more with approximately eleven thousand observations. This dataset comes from Kaggle, a platform for everyone to compete, collaborate, and learn, and was provided by user Rupinder Singh Rana. It includes different features of cars along with the price from 1990 to 2017, providing a historical overview on pricing trends as car advanced in technology. The dataset consists of numerical values, such as engine hp, highway mpg, and more and categorical values, such as engine fuel type, transmission type, and more. 

(https://www.kaggle.com/datasets/rupindersinghrana/car-features-and-prices-dataset).  

# Explanatory Data Analysis 

In this section, I am conducting Explanatory Data Analysis (EDA) to better understand the data and the variables that may influence the `MSRP` of cars. This is the stage where we are given insights into the structure and characteristics of the data, for instance missing values and outliers. 

I will provide visuals on missing values, correlation matrices, and boxplots to give us an idea of what the data is providing. 

## Loading Data 

Firstly, I am going to load the dataset.
```{r}
library(dplyr)
car_data <- read.csv("~/Desktop/data.csv")
car_data <- car_data %>% 
  mutate(Category = ifelse(Make %in% c("BMW", "Audi", "Mercedes-Benz", "Volvo", "Ferrari", 
          "Alfa Romero", "McLaren", "Maybach","Porsche", "Saab", "Cadillac", "Bentley", "Lamborghini", 
          "Lincoln", "Rolls-Royce", "Buick", "Maserati","Lexus", "Aston Martin", "Land Rover",
          "Lotus", "Infiniti", "Genesis", "Bugatti"),
                         "Luxury",
                         "Non-Luxury"))

head(car_data)
```

Now, I am going to get the dataset's dimensions. This tells us the number of rows and columns.

```{r}
dim(car_data)
```

This dataset includes 11,914 rows and 17 columns. The 11,914 rows are different cars and 16 columns are the car's features and the remaining one is the `MSRP`. Here are the car's features and price:

- `Make`: The company that produces the vehicle

- `Model`: The product name of the vehicle used by the manufacturer

- `Year`: The year the vehicle was made 

- `Engine.Fuel.Type`: The type of fuel the vehicle uses

- `Engine.HP`: The engine's horse power

- `Engine.Cylinders`: Number of cylinders

- `Transmission.Type`: The type of transmission the vehicle has

- `Driven_Wheels`: The type of wheels the vehicle has

- `Number.of.Doors`: The number of doors the vehicle has

- `Market.Category`: The classification of the vehicle 

- `Vehicle.Size`: The size of the vehicle

- `Vehicle.Style`: Type of vehicle

- `highway.MPG`: How many miles a vehicle can travel on a gallon of gas while driving on the freeway

- `city.MPG`: How many miles a vehicle can travel on a gallon of gas while driving in the city

- `Popularity`: Popularity of the vehicle

- `MSRP`: Price of the vehicle in ($)

- `Category`: Whether a a car is luxury or non-luxury

After finding out how many observations we have, we want to know how much missing data there is. It is important to know how much data we are missing as it can lead to a reduced sample size, biased results, or misleading results. 

## Tidying Data 

### Missing Data

```{r}
missing <- is.na(car_data)

total_missing <- sum(missing)
print(total_missing)
```

The dataset is missing 105 observations. Let us see the percentage by variable. 

```{r}
library(naniar)
vis_miss(car_data)

colSums(is.na(car_data))
```

Looking at the graph, we can see there are a few missing observations for `Engine.HP`, `Engine.Cylinders`, `Number.of.Doors`. Specifically, there are 69 missing observations for `Engine.HP`, 30 for `Engine.Cylinders`, and 6 for `Number.of.Doors`. While skimming through the dataset, I noticed the function viss_miss did not catch other missing values due to how it was represented. 

```{r}
library(tidyr)

updated_car_data <- car_data %>% 
  drop_na(Engine.HP, Engine.Cylinders, Number.of.Doors) 

NA_car_data <- subset(updated_car_data, !( Engine.Fuel.Type == "" | Engine.Cylinders == 0 
                                           | Transmission.Type == "UNKNOWN"))
```

Upon reviewing the dataset, I decided on removing the observations with missing data in `Engine.HP`, `Engine.Cylinders`, and `Number.of.Doors` since we could afford to remove the missing observations. After reviewing the updated set with the missing values removed, I noticed that some rows had a value of 0 in `Engine.Cylinders`, which seemed incorrect. As a result, I chose to remove the rows.  

In the `Market.Category` variable there are approximately three thousand observations marked as "N/A" that the program did not detect. To evaluate the significance of the missing data points, I conducted two trials: one where missing observations were removed (No NA trial) and the other where missing observations were kept without imputation (NA trial). 

After training and comparing the model results on both trials, the NA trial demonstrated better performance across RMSE. This tell us that retaining the missing values contributes to the predictive power of the models. I chose not to impute the missing data points because doing so will introduce bias which would lead to false conclusions and be misleading for our inference conclusions. 

## Outliers

To better understand the data, we're going to visualize predictors with a wide range of values. This is important as they create outliers in the data. Outliers are data values that significantly deviate from the majority of other values. 

```{r}
ggplot(NA_car_data, aes(MSRP)) +
  geom_boxplot() + coord_flip() + labs(x = "MSRP (in thousands)")
ggplot(NA_car_data, aes(highway.MPG)) +
  geom_boxplot() + coord_flip() + labs(x = "Highway MPG")
ggplot(NA_car_data, aes(Engine.HP)) +
  geom_boxplot() + coord_flip() + labs(x = "Engine Horse Power")

```

As you can see the `MSRP` boxplot reveals significant outliers, with values ranging from approximately \$250,000 to \$2,000,000. In the `highway.MPG` variable, there is a single extreme outlier approximately 350 MPG, which is unrealistic. Lastly, `Engine.HP` has outliers ranging from 500 to 1,000 horse power. 

Let's transform the data to exclude the extreme outliers.
```{r}
IQR_HP_NA <- IQR(updated_car_data$Engine.HP)
lower_HP_NA <- quantile(updated_car_data$Engine.HP, 0.25) - 2.0 * IQR_HP_NA
upper_HP_NA <- quantile(updated_car_data$Engine.HP, 0.75) + 2.5 * IQR_HP_NA

IQR_highway_NA <- IQR(updated_car_data$highway.MPG)
lower_highway_NA <- quantile(updated_car_data$highway.MPG, 0.25) - 3.0 * IQR_highway_NA
upper_highway_NA <- quantile(updated_car_data$highway.MPG, 0.75) + 4.0 * IQR_highway_NA

IQR_MSRP_NA <- IQR(updated_car_data$MSRP)
lower_MSRP_NA <- quantile(updated_car_data$MSRP, 0.25) - 3.0 * IQR_MSRP_NA
upper_MSRP_NA <- quantile(updated_car_data$MSRP, 0.75) + 2.0 * IQR_MSRP_NA

outliers_removed_car_NA <- NA_car_data %>% 
  filter(Engine.HP >= lower_HP_NA & Engine.HP <= upper_HP_NA) %>% 
  filter(highway.MPG >= lower_highway_NA & highway.MPG <= upper_highway_NA) %>% 
  filter(MSRP >= lower_MSRP_NA & MSRP <= upper_MSRP_NA)
```


```{r}
#install.packages("ggplot2")
library(ggplot2)

ggplot(outliers_removed_car_NA, aes(MSRP)) +
  geom_boxplot() + coord_flip() + labs(x = "MSRP (in thousands)")
ggplot(outliers_removed_car_NA, aes(highway.MPG)) +
  geom_boxplot() + coord_flip() + labs(x = "Highway MPG")
ggplot(outliers_removed_car_NA, aes(Engine.HP)) +
  geom_boxplot() + coord_flip() + labs(x = "Engine Horse Power")
```

Above are the same boxplots after applying the transformations to the data. Some outliers remain which are intentionally retain to preserve the high range of values. 

## Transformation

```{r}
library(forcats)
converted_car_data_NA <- outliers_removed_car_NA %>% 
    mutate(
        Market.Category = fct_other(Market.Category, 
            keep = c("Performance", "Hatchback", "Crossover"), 
            other_level = "Other"),
        Vehicle.Style = fct_lump(Vehicle.Style, n = 11, other_level = "Other"),
        Make = fct_lump(Make, n = 32, other_level = "Other"),
        MSRP = MSRP / 1000,
        Vehicle.Size = factor(Vehicle.Size),
        Vehicle.Style = factor(Vehicle.Style),
        Engine.Fuel.Type = factor(Engine.Fuel.Type),
        Driven_Wheels = factor(Driven_Wheels),
        Transmission.Type = factor(Transmission.Type),
        Make = factor(Make),
        Model = factor(Model),
        Category = factor(Category)
    )
```

Here, I am lumping categories in `Market.Category` that are not "Performance", "Hatchback", or "Crossover. I picked these 3 as were in the top 10 with  the highest number of observations. I also lumped categories in `Vehicle.Style` and `Make` keeping the top 11 and 32 and grouping the other observations as "Other." I divided the `MSRP` by a thousand to have a greater range in y for the visual data. Lastly, I factored only the categorical values. 

## Visual Data

We will explore some relationships between the predictors and MSRP.

### MSRP

```{r}
ggplot(converted_car_data_NA, aes(x = MSRP)) +
  geom_histogram(bins = 40, fill = "skyblue", color = "black") + labs(x = "MSRP (in thousands)", y = "Count")
```

The histrogram visualizes the distribution of `MSRP` with a range from close to \$0 up to at least \$85,000. The distribution is right skewed meaning most vehicles have a lower MSRP. The largest mode is near \$0 to \$5,000, this indicates there is a significant amount of cars with low MSRP and there is a another cluster around \$20,000 to \$30,000. 
 

### MSRP vs Transmission Type 

```{r}
library(dplyr)
library(ggplot2)

#updated_car_data <- updated_car_data %>% 
  #mutate(Transmission.Type = factor(Transmission.Type),
         #Category = factor(Category))

ggplot(converted_car_data_NA, aes(x = Transmission.Type, y = MSRP)) + 
  geom_boxplot(fill = "slateblue", alpha = 0.2) + xlab("Transmission Type") + 
  ylab("MSRP (in thousands)") + ylim(0,75)

```

The boxplot visualizes the distribution of `MSRP` across different `Transmission.Type` categories.The median MSRP varies among transmission types, with "AUTOMATIC" and "AUTOMATED_MANUAL" having equal highest median. The interquartile range (IQR), which is the spread of the data, reveals that "DIRECT_DRIVE" has the narrowest price range, indicating consistency in pricing for this transmission type.

Examining the whiskers,"MANUAL" has a longer spread toward the maximum, while "AUTOMATIC" displays an equal amount of spread toward the maximum and minimum values. Additionally, three out of the four transmissions have a good amount of outliers, highlighting the presence of cars with MSRP significantly above or below their boxplots.  

### MSRP vs Make

```{r}
common_10 <- converted_car_data_NA %>% 
  filter(Make %in% c("BMW", "Mercedes-Benz", "Lincoln", "Lexus", "Infiniti", "Honda", "Toyota", 
                   "Nissan", "Ford", "Subaru"))
ggplot(common_10, aes(x = Make, y = MSRP)) +
  geom_boxplot(fill = "purple") + xlab("Car Make") + ylab("MSRP (in thousands)") + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```

The boxplots compare the `MSRP` distributions for five luxury and five non-luxury car `Make`s. As we can see, Mercedes-Benz has a greater IQR than the other car makes. 

- Mercedes-Benz: The median `MSRP` is approximately \$42,000, with an IQR ranging from around \$5,000 and \$55,000. 

- Toyota: The median `MSRP` is around \$30,000, with an IQR ranging from around \$23,000 and \$34,000 with an outlier around \$64,000. 

### MSRP vs Category

![image](~/Desktop/nonluxury&luxury.webp)
(https://www.carpro.com/blog/toyota-bmw-are-the-most-shopped-luxury-and-non-luxury-car-brands)

```{r}
ggplot(converted_car_data_NA, aes(x = Category, y = MSRP)) +
  geom_boxplot(fill = "coral") + ylim(0, 90) + ylab("MSRP (in thousands)")
```

The boxplots compare the distribution of `MSRP` to `Category`. This shows the distribution within luxury and non-luxury. 

- Luxury: The median `MSRP` is approximately \$40,000, with an IQR range from around \$35,000 to \$52,000. The minimum `MSRP` is about $10,000 and maximum (excluding outliers) is around \$82,000. There are various amounts of outliers ranging from approximately \$83,000 to \$86,000. 

- Non-luxury: The median `MSRP` is approximately \$26,000, with an IQR from around \$18,000 to \$33,000. The minimum `MSRP` is about \$3,000 and maximum (excluding outliers) is around \$52,000. There are various amounts of outliers ranging from about \$53,000 to \$85,000. 

### MSRP vs Year

```{r}
ggplot(converted_car_data_NA, aes(x = factor(Year), y = MSRP)) +
  geom_point(alpha = 0.7, color = "darkgreen")  +
  labs( x = "Year", y = "MSRP (in thousands)") + ylim(0,200) + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```

Looking at the scatterplot of the distribution of `MSRP` and `Year`, we see an upward trend in the `MSRP` values over the years. After the year 2000, the prices dramatically increased suggesting vehicle prices increase every year. The significant increase could indicate factors such economic changes, advancements in technology, inflation, or an increase in demand.

### Correlation Matrix 

```{r}
library(corrplot)
converted_car_data_NA %>% 
  select(where(is.numeric)) %>% 
  cor() %>% 
  corrplot(addCoef.col = 1, , number.cex = 0.6)
  
```

Now, we are going to explore the relationships between pairs of variables in the dataset. As a reminder, correlation coefficients range from -1 to 1, where values close to -1 or 1 indicating a stronger relationship. 

- City MPG and Highway MPG: A very strong positive correlation (r = 0.91) shows that cars with better fuel efficiency in city conditions will also be efficient in highway conditions.
- Engine Cylinders and Engine HP: There is a strong positive correlation (r = 0.71), indicating vehicles with more engine cylinders tend to have higher horsepower.
- Engine Horsepower and MSRP: The strong positive correlation (r = 0.75) indicates vehicles with higher horsepower tend to have higher prices. 
- Engine Cylinders and Fuel Efficiency: There is a strong negative correlation between engine cylinders and both city mpg (r = -0.68) and highway mpg 
(r = -0.70). This implies vehicles with more cylinders tend to have worse fuel efficiency in both city and highway conditions.
- Engine Horsepower and Fuel Efficiency: There is moderate negative correlation between engine horsepower and both city mpg (r = -0.49) and highway mpg
(r = -0.44). This implies vehicles with more horsepower results in lower fuel efficiency.

# Setting up Models 

It is time to set up our models. This is where split our data and fit the training data into different models to see which one is the best to predict the `MSRP`. Before fitting, we have to create the recipe and the folds for k-fold cross validation.

## Data Split

The first step would be to split the data into training and testing data sets. The training data is used to train the model to estimate `MSRP` based on observed data (pg. 21, ISRL book). The testing data, which contains the unseen observations, is used to test the model's accuracy (pg. 30). I have chosen a 70/30 split, assigning 70% to the training set and 30% to the testing set. This split will help the model train while reserving a portion to test the accuracy on the remaining observations, helping to prevent overfitting. The split is stratified on the `MSRP` variable to ensure the same distribution on both the training and testing sets, and a random seed to reproduce my results consistently.


```{r}
set.seed(314)

NA_car_split <- initial_split(converted_car_data_NA, prop = 0.7, 
                           strata = MSRP)
NA_car_training <- training(NA_car_split)
NA_car_testing <- testing(NA_car_split)

```

To make sure the data is split evenly, we need to check the dimensions.

```{r}
dim(NA_car_training)
dim(NA_car_testing)
```

The observations for the training and testing data sets are adequate. 

## Recipe

Throughout our analysis, different recipes will be used for each model to accommodate their specific requirements. For stepwise linear regression,  10 predictors will be used: `Make`, `Year`, `Engine.Fuel.Type`, `Engine.HP`, `Transmission.Type`, `Driven_Wheels`, `Market.Category`, `Vehicle.Size`, `Vehicle.Style`, and `city.mpg`. 

For lasso regression, random forest, and PCA regression, all available predictors will be used. In addition to the 10 mentioned above, the other predictors are: `Model`, `Engine.Cylinders`, `Number.of.Doors`, `highway.MPG`, `Popularity`, and `Category`.

To deal with the number of categories within Make and Model for lasso regression, random forest, and PCA regression, levels with fewer than 10% of the data were grouped into an "Other" category. This reduces complexity for the models and not produce unreliable coefficient estimates.

```{r}
#Stepwise Regression
NA_stepwise_car_recipe <- recipe(MSRP ~ Make + Year + Engine.Fuel.Type + Engine.HP + 
                       Transmission.Type + Driven_Wheels + Market.Category + Vehicle.Size + 
                       Vehicle.Style + city.mpg, data = NA_car_training) %>% 
  step_dummy(Make, Engine.Fuel.Type, Transmission.Type, 
             Driven_Wheels, Market.Category, 
            Vehicle.Style, Vehicle.Size)

#Lasso Regression
lasso_recipe_NA <- recipe(MSRP ~ ., data = NA_car_training) %>% 
  step_other(Model, threshold = 0.10) %>%
  step_dummy(Make, Model, Engine.Fuel.Type, Transmission.Type, 
             Driven_Wheels, Market.Category, Vehicle.Style, Vehicle.Size, Category) %>%
  step_zv(all_predictors()) %>% 
  step_center(all_predictors()) %>% 
  step_scale(all_predictors())

#Random Forest
rf_recipe_NA <- recipe(MSRP ~ ., data = NA_car_training) %>% 
  step_other(Model, threshold = 0.10) %>%
  step_dummy(Make, Model, Engine.Fuel.Type, Transmission.Type, 
             Driven_Wheels, Market.Category, Vehicle.Style, Vehicle.Size, Category) %>%
  step_zv(all_predictors())

#PCA Regression
pca_recipe_NA <- recipe(MSRP ~ ., data = NA_car_training) %>% 
  step_other(Make, Model, threshold = 0.10) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_zv(all_predictors()) %>% 
  step_center(all_predictors()) %>% 
  step_scale(all_predictors()) %>% 
  step_pca(all_predictors(), num_comp = 5)
```

## K-Fold Cross Validation

We are using K-Fold Cross Validation, which randomly divides the set of observations into 5 folds that are approximately equal size (pg. 203). The first fold is treated as a validation set and the model is trained on the remaining k-1 folds. A performance metric, such as RMSE or $R^2$, is then computed from the remaining observations used to evaluate the model. This process is repeated 5 times with a different group of observations used as a validation set. Finally, it then averages the test error across all folds.

```{r}
set.seed(314)
NA_car_folds <- vfold_cv(NA_car_training, v = 5, strata = MSRP)
```

# Model Building

In this section, we implement several models to predict `MSRP` and to learn more about the variables influencing car prices. We will build the model using the tidied data from earlier, prepare the training data, select appropriate models, train the models on the data, fine tune the parameters, and evaluate each model's performance. We then choose the model with the best metric. I will be using Root Mean Square Error (RMSE) as my metric for all of the models. RMSE is used to quantify how far predictions are from the true values. The lower the RMSE, the closer the predictions are from the true values and the higher the RMSE, the farther away the predictions are from the true values.

As seen above in the recipe building, I will be using Stepwise Linear Regression, Lasso Regression, Random Forest, and PCA Regression. Each model contributes its own unique strengths: stepwise regression examines the significance of each variable, lasso eliminates the less important predictors in the model by shrinking their coefficients by zero, random forest provides information about the significance of the predictors in the data, and PCA regression reduces dimensionality. 

stepwise - https://www.investopedia.com/terms/s/stepwise-regression.asp
lasso - https://www.ibm.com/topics/lasso-regression#:~:text=Lasso%20regression%20can%20help%20to,important%20features%20from%20the%20model.&text=The%20bias%20introduced%20by%20the,shrink%20the%20coefficients%20towards%20zero.
random f - https://aiml.com/what-are-the-advantages-and-disadvantages-of-random-forest/
pca - pg. 253

## Fitting the Models 

The models used are going to have the same procedure. Here are the steps:

1. Set up the model by specifying what type of model, engine, and mode. For Lasso and Random Forest, add in hyperparameters. 
```{r}
#Stepwise Regression
NA_stepwise_model <- linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode("regression")

#Lasso Regression
lasso_model_NA <- linear_reg(
  mixture = 1,
  penalty = tune()
) %>% 
  set_mode("regression") %>% 
  set_engine("glmnet")

#Random Forest
rf_model_NA <- rand_forest(
  mtry = tune(),
  trees = tune(),
  min_n = tune()
) %>% 
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("regression")

#PCA Regression 
pca_model_NA <- linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode("regression")

```

2. Set up the workflow, add in the mode and recipe.
```{r}
#Stepwise Regression
NA_stepwise_workflow <- workflow() %>% 
  add_model(NA_stepwise_model) %>% 
  add_recipe(NA_stepwise_car_recipe)

#Lasso Regression
lasso_workflow_NA <- workflow() %>% 
  add_model(lasso_model_NA) %>% 
  add_recipe(lasso_recipe_NA)

#Random Forest
rf_workflow_NA <- workflow() %>% 
  add_model(rf_model_NA) %>% 
  add_recipe(rf_recipe_NA)

#PCA Regression
pca_workflow_NA <- workflow() %>% 
  add_model(pca_model_NA) %>% 
  add_recipe(pca_recipe_NA)
```

3. Create a grid and tune for those with hyperparameters in step 1. 
```{r}
#No tuning for Stepwise Regression

#Lasso Regression
lasso_grid_NA <- grid_regular(penalty(), levels = 30)

#Random Forest
#rf_grid_NA <- grid_regular(mtry(range = c(1,5)),
                        #trees(range = c(200, 650)),
                        #min_n(range = c(5, 15)),
                        #levels = 10)

#No tuning for PCA Regression
```

4. Tune the models with grids by adding in the workflow, k-fold cross validation, and grid. For the other models, fit the workflow and k-folds.
```{r}
#Stepwise Regression
NA_stepwise_fit <- fit_resamples(NA_stepwise_workflow, NA_car_folds)

#Lasso Regression
lasso_tunegrid_NA <- tune_grid(
  lasso_workflow_NA,
  resamples = NA_car_folds,
  grid = lasso_grid_NA
)

#Random Forest
#rf_tune_NA <- tune_grid(
  #rf_workflow_NA,
  #resamples = NA_car_folds,
  #grid = rf_grid_NA,
  #metrics = metric_set(rmse)
#)

#PCA Regression
pca_fit_NA <- fit_resamples(pca_workflow_NA, NA_car_folds)
```

5. Save the tuned model. 
```{r}
#Lasso
save(lasso_tunegrid_NA, file = "lasso_tunegrid_NA.rda")

#Random Forest
save(rf_tune_NA3, file = "~/Desktop/rf_tune_NA3.rda")
```

6. Load the saved files
```{r}
#Lasso 
load(lasso_tunegrid_NA)

#Random Forest
load("~/Desktop/rf_tune_NA3.rda")
```

7. Collect the metrics from each model, filter rmse, arrange from ascending order, and slice to show the first result.
```{r}
#Stepwise Regression
NA_stepwise_metrics <- collect_metrics(NA_stepwise_fit) %>% 
  filter(.metric == "rmse") %>% 
  arrange(mean) %>% 
  slice_head(n=1)

#Lasso Regression
lasso_metrics_NA <- collect_metrics(lasso_tunegrid_NA )%>%
  filter(.metric == "rmse") %>% 
  arrange(mean) %>% 
  slice_head(n=1)

#Random Forest
rf_metrics_NA3 <- rf_tune_NA3 %>%
  collect_metrics() %>% 
  filter(.metric == "rmse") %>% 
  arrange(mean) %>% 
  slice_head(n=1)

#PCA Regression
pca_metrics_NA <- collect_metrics(pca_fit_NA) %>% 
  filter(.metric == "rmse") %>% 
  arrange(mean) %>% 
  slice_head(n=1)
```


## Model Autoplot 

Stepwise Regression: 
Stepwise Regression does not have model autoplot but was used as an inferential model to identify significant predictors of MSRP. The model began with all predictors and removed non-significant predictors using backward selection. The final model included `Make`, `Year`, `Engine.Fuel.Type`, `Engine.HP`,`Transmission.Type`, `Driven_Wheels`,`Market.Category`,`Vehicle.Size`,`Vehicle.Style`, and `city.mpg`. The model was tuned using 5-fold cross validation and achieved an RMSE of 6.626349.

### Lasso
```{r}
#Lasso
autoplot(lasso_tunegrid_NA, metric = 'rmse')
```

Lasso Regression: 
Lasso Regression was used as a predictive model. The hyperparameter `penalty()` was tuned using 5-fold cross validation over a grid of 30 levels. The RMSE is low and stable when it is around 1e-08 to 1e-02. After 1e-02, the RMSE increases significantly. The best penalty was 1.000000e-10 with an RMSE of 6.583335.

### Random Forest
```{r}
#Random Forest
autoplot(rf_tune_NA3)
```

Random Forest: 
Random Forest was used to optimize prediction accuracy. The hyperparameters `mtry()`, `trees()`, and `min_n()` were tuned using grid regular: `mtry()` tested values from 1 to 5, `trees()` tested values from 200 to 650, and `min_n()` tested values from 5 to 15. The hyperparameters were tuned over a grid of 10 levels and 5-fold cross validation. The best `mtry` was 5, `trees` was 500, and `min_n` was 5 with an RMSE of 4.628497. As we can see, all of the minimal node sizes tend to have the same RMSE in each graph and the number of trees seems to not have an effect either. 

PCA Regression: 
PCA Regression was used to reduce dimensionality while keeping the important predictors. It had a RMSE of 9.558593.

# Model Results

We are going to compare the results of the models and see which one has the best RMSE.

```{r}
stepwise_results <- tibble(
  Model = "Stepwise Regression",
  RMSE = 6.626349
)
lasso_results <- tibble(
  Model = "Lasso Regression",
  RMSE = 6.583335
)
randomf_results <- tibble(
  Model = "Random Forest",
  RMSE = 4.628497
)
pca_results <- tibble(
  Model = "PCA Regression",
  RMSE = 9.558593
)
all_metrics <- bind_rows(stepwise_results, lasso_results, randomf_results, pca_results)

print(all_metrics)
```

As we can see Random Forest performed the best with the lowest RMSE. Our second best was Lasso Regression, third was Stepwise Regression, and last was PCA Regression. Something we can take from this is that Random Forest has the best prediction accuracy and may indicate the data is nonlinear as it can model nonlinear relationships between the features. (https://docs.heavy.ai/heavyml-beta/regression-algorithms/random-forest-regression)

As mentioned above our best parameters were 500 trees and 5 minimal nodes with 5 predictors and an RMSE of 4.628497. 

## Fitting Best Model

Now that we have identified the model with the best RMSE, it will be used to make predictions on the testing data. The selected model will be trained  on the full training dataset using the best parameters obtained during tuning. 

```{r}
best_rf1 <- select_best(rf_tune_NA3, metric = "rmse")

rforest_final <- finalize_workflow(rf_workflow_NA3, best_rf1)

rforest_final_fit <- fit(rforest_final, data = NA_car_training)
```


## Testing Model

It is now time to evaluate our best model on unseen observations using the testing data. This step will demonstrate how well the model can predict unseen observations.

```{r}
augment_rf <- augment(rforest_final_fit, new_data = NA_car_testing)

metric_rf <- augment_rf %>% 
  metrics(truth = MSRP, estimate = .pred)

metric_rf %>% filter(.metric == "rmse")

```
Woohoo! Our testing RMSE is lower than our training RMSE. This means the prediction was on average 4.487 away from the true value. 

```{r}
ggplot(augment_rf, aes(x=.pred, y = MSRP)) + geom_point(alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = "blue", linetype = "dashed") +
  labs(x = "Predicted MSRP (in thousands)", 
       y = "Actual MSRP (in thousands)",
       title = "Predicted Values vs True Values")

```

The testing model performed well in predicting MSRP values, as most of the predicted values lie close to the diagonal slope. However, the predictions on the extreme ends of the MSRP range are further away from the line. This indicates the model was well-trained for cars priced between \$20,000 to \$45,000, but struggles with predicting MSRP values for cars priced significantly lower or higher. To get a more accurate predictions, we might need to focus more on either the lower or upper tails. 

## Variable Importance Plot

This plot demonstrates which variables were the most important on the best-performing random forest model. 

```{r}
rforest_final_fit %>% 
  extract_fit_parsnip() %>% 
  vip(geom = "col", aesthetics = list(fill = "palegreen3", color = "black"))

```

The two most significant features were `Year` and `Engine.HP`. This suggests that newer cars, which often have more advanced technology, tend to have higher MSRPs. Similarly, cars with better performance give a better driving experience which demand higher MSRPs. Additionally other significant features include `Engine.Fuel.Type_regular.unleaded`, and `Category_Non.Luxury`, which demonstrates the influence of fuel type and affordability. These features indicate the importance of technological features, driving experience, fuel availability, and affordability when it comes to MSRP.  

# Conclusion

Through this project we made a goal on predicting the MSRP based on car features. From there we cleaned our data and visualized our data to see what the predictors were telling us. We continued on with creating our training and testing sets and used models to see which one was the best by seeing which one had the lowest RMSE. The PCA Regression had the highest RMSE with a value of 9.558593, Stepwise Regression had the second highest RMSE with a value of 6.626349, third was Lasso Regression with a value of 6.583335, and last with the lowest RMSE was Random Forest with a value of 4.628497. We concluded Random Forest was the best model and used it on our testing set. The testing RMSE came out better than our training RMSE and also gave us which features are important when predicting RMSE. The significant features were year, engine horsepower, regular unleaded engine fuel type, and non luxury category.

I was not surprised about Random Forest's performance because it has a high accuracy for predictions and it is resilient to noise. I was surprised by Stepwise Regression as it wasn't that far off from the RMSE value of Random Forest. 

# Sources
Kaggle. Car Features and Prices Dataset, Version 1.0, 2024, Kaggle, https://www.kaggle.com/datasets/rupindersinghrana/car-features-and-prices-dataset/data. 